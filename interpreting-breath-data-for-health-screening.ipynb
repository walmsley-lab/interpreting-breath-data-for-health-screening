{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9619287,"sourceType":"datasetVersion","datasetId":5870732}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Interpreting Breath Data for Health Screening\n\nhttps://github.com/walmsley-lab/interpreting-breath-data-for-health-screening","metadata":{}},{"cell_type":"markdown","source":"## Introduction\n\nHealthcare analytics has traditionally relied on structured clinical data such as diagnosis codes, procedures, and utilization records to evaluate outcomes, costs, and care quality at scale. Experience across provider-facing medical devices and payer-side analytics highlights both the power and the limits of this paradigm: while historical claims and encounter records support population-level insights, they often capture downstream consequences rather than early physiological signals.\n\nAs healthcare data sources expand, modalities such as wearables, low-cost biological assays, and chemical sensing technologies introduce opportunities for earlier and more subtle detection of disease processes. Among these, electronic nose (e-nose) technologies, arrays of broadly sensitive gas sensors, have shown promise in detecting disease-associated changes in volatile organic compounds (VOCs) linked to metabolic, respiratory, and oncologic conditions. Rather than identifying individual biomarkers, e-nose systems rely on multivariate response patterns that reflect shifts in underlying physiology.\n\nThis project explores whether deep learning methods can reliably extract such weak or early signals from breath-based sensor data using a principled, interpretable modeling pipeline. The objective is not to build a clinically deployable diagnostic tool, but to establish methodological foundations for representation learning, stability analysis, and interpretability in scent-based health screening. By grounding the analysis in realistic datasets and emphasizing transparency over complexity, this work aims to inform future applications of sensor-driven insights in value-based care and early-detection contexts.","metadata":{}},{"cell_type":"markdown","source":"## Data Selection and Rationale\n\nPublicly available scent- and breath-based datasets vary widely in sensing modality, scale, and clinical grounding. Some datasets capture breath VOCs using gas chromatography–mass spectrometry (GC–MS), offering compound-level resolution but at the cost of small sample sizes, heavy preprocessing requirements, and limited reproducibility in open settings. Others rely on off-the-shelf electronic sensors that provide coarse but scalable signal patterns closer to what might be feasible in real-world deployments.\n\nAfter surveying multiple candidates, including GC–MS breathomics datasets, electronic nose datasets for lung cancer screening, diabetes detection, and generic sensor drift studies, this project adopts the e-Nose Sensor Dataset for Predicting Human Diseases hosted on Kaggle as its primary data source. This dataset represents the largest publicly accessible collection of breath-related electronic sensor measurements paired with disease labels, comprising approximately 1,000 samples with balanced diabetic and control classes. Each sample consists of a single snapshot of six low-cost metal-oxide gas sensors with overlapping sensitivity profiles.\n\nThis choice reflects a deliberate tradeoff. While GC–MS datasets provide richer chemical specificity, their limited scale introduces instability and risks overfitting in deep learning workflows. In contrast, the selected e-nose dataset supports robust exploratory analysis, representation learning, and cross-validation within the constraints of an academic setting. The dataset’s structure built static, tabular, multivariate sensor responses mapped to health status, closely mirrors realistic screening scenarios where cost, noise, and scalability matter as much as signal strength.","metadata":{}},{"cell_type":"markdown","source":"## Problem Framing\n\nEach observation in the dataset corresponds to a single breath or scent measurement captured by a six-sensor array and paired with an associated health label. The sensors themselves are not disease-specific; rather, each exhibits cross-sensitivity to overlapping subsets of volatile organic compounds (VOCs). As a result, informative signal arises from the collective response pattern across the sensor array rather than from any individual measurement.\n\nAccordingly, the modeling task is framed as a screening and pattern-discovery problem. The objective is to assess whether disease-associated chemical signatures can be learned from multivariate sensor responses using deep learning methods that respect the static, low-dimensional structure of the data, rather than imposing inappropriate spatial or temporal assumptions.","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# CELL 1: SETUP AND IMPORTS\n# ============================================================================\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.linalg import cholesky\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntry:\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    from torch.utils.data import DataLoader, TensorDataset\n    import torch.nn.functional as F\n    \n    # Set seeds for reproducibility\n    SEED = 42\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(SEED)\n        torch.backends.cudnn.deterministic = True\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \nexcept ImportError as e:\n    print(f\"Failed to import PyTorch: {e}\")\n    print(\"Please check your PyTorch installation\")\nexcept Exception as e:\n    print(f\"An unexpected error occurred: {e}\")\n\n# Scikit-learn (for preprocessing and metrics)\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import (classification_report, confusion_matrix, \n                             roc_curve, auc, accuracy_score, f1_score,\n                             precision_score, recall_score, silhouette_score)\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Set seeds for reproducibility\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(SEED)\n    torch.backends.cudnn.deterministic = True\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Plotting style\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette(\"husl\")\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Device: {device}\")\nprint(f\"CUDA Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\nprint(f\"NumPy version: {np.__version__}\")\nprint(f\"Random Seed: {SEED}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:32:31.059178Z","iopub.execute_input":"2025-12-09T18:32:31.059559Z","iopub.status.idle":"2025-12-09T18:32:31.076306Z","shell.execute_reply.started":"2025-12-09T18:32:31.059532Z","shell.execute_reply":"2025-12-09T18:32:31.075196Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 2: DATA LOADING\n# ============================================================================\n\nimport glob\nimport os\n\n# Load data\ndata_source = \"/kaggle/input/enose-sensor-dataset-for-predicting-human-diseases/enose_dataset_to_predict_human_disease.csv\"\ndf = pd.read_csv(data_source)\n\n# Note:\n# During the data exploration phase a novel concept was introduced that we might use synthetic data.\n# Doing so would allow us to develop, test, and demo our algorithm though insights would be lackluster.\n# This data generation is possible by including a sensor correlation matrix capturing real signal correlations.\n# Imagine, hydrocarbons might trigger TGS2610 and TGS2611 captured with high correlation score.\n# This would be assembled using the Cholesky method. Demonstration omitted for brevity.\n\n# Standardize column names\nif 'Subjek' in df.columns:\n    df = df.rename(columns={'Subjek': 'Diagnosis'})\n\n# Identify sensor columns\nexclude_cols = ['Time(s)', 'Number', 'Diagnosis']  # Fixed: 'Subjek' → 'Diagnosis' (post-rename)\nsensor_cols = [col for col in df.columns if col not in exclude_cols \n               and df[col].dtype in ['float64', 'int64', 'float32', 'int32']]\n\n# Data Quality Checks\n\nprint(\"=\"*60)\nprint(\"DATASET SUMMARY\")\nprint(\"=\"*60)\nprint(f\"Source: {data_source.split('/')[-1]}\")  # Just filename\nprint(f\"Shape: {df.shape[0]} samples × {df.shape[1]} columns\")\nprint(f\"Sensor Columns ({len(sensor_cols)}): {sensor_cols}\")\n\nprint(\"\\n\" + \"-\"*60)\nprint(\"DIAGNOSIS DISTRIBUTION\")\nprint(\"-\"*60)\nprint(df['Diagnosis'].value_counts().to_string())\nprint(f\"Total: {df['Diagnosis'].value_counts().sum()}\")\n\nprint(\"\\n\" + \"-\"*60)\nprint(\"DATA QUALITY CHECKS\")\nprint(\"-\"*60)\n\n# Check for nulls\nnull_counts = df[sensor_cols].isnull().sum()\ntotal_nulls = null_counts.sum()\nif total_nulls == 0:\n    print(\"No missing values in sensor columns\")\nelse:\n    print(f\"Missing values found: {total_nulls} total\")\n    print(null_counts[null_counts > 0].to_string())\n\n# Check for non-numeric values in sensor columns\nnon_numeric_issues = []\nfor col in sensor_cols:\n    if not pd.api.types.is_numeric_dtype(df[col]):\n        non_numeric_issues.append(col)\n    elif df[col].apply(lambda x: isinstance(x, (int, float)) or pd.isna(x)).all() == False:\n        non_numeric_issues.append(col)\n\nif len(non_numeric_issues) == 0:\n    print(\"✓ All sensor columns are numeric\")\nelse:\n    print(f\"⚠ Non-numeric data in: {non_numeric_issues}\")\n\n# Check for suspicious values (negatives, infinities)\nhas_negative = (df[sensor_cols] < 0).any().any()\nhas_inf = np.isinf(df[sensor_cols].values).any()\n\nif not has_negative:\n    print(\"✓ No negative sensor values\")\nelse:\n    neg_cols = [col for col in sensor_cols if (df[col] < 0).any()]\n    print(f\"⚠ Negative values in: {neg_cols}\")\n\nif not has_inf:\n    print(\"✓ No infinite values\")\nelse:\n    print(\"⚠ Infinite values detected\")\n\n# Quick stats\nprint(\"\\n\" + \"-\"*60)\nprint(\"SENSOR STATISTICS\")\nprint(\"-\"*60)\nprint(df[sensor_cols].describe().round(1).to_string())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:32:31.077976Z","iopub.execute_input":"2025-12-09T18:32:31.078311Z","iopub.status.idle":"2025-12-09T18:32:31.153987Z","shell.execute_reply.started":"2025-12-09T18:32:31.078288Z","shell.execute_reply":"2025-12-09T18:32:31.152851Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# Sensor Distributions by Class\n# ============================================================================\n\nfig, axes = plt.subplots(1, 6, figsize=(18, 4), sharey=True)\n\ncolors = {'Normal': '#3498db', 'Diabetes': '#e74c3c'}\n\nfor ax, col in zip(axes, sensor_cols):\n    for label in ['Normal', 'Diabetes']:\n        subset = df[df['Diagnosis'] == label][col]\n        ax.hist(\n            subset,\n            bins=30,\n            density=True,\n            alpha=0.6,\n            color=colors[label],\n            label=label\n        )\n    ax.set_title(col, fontsize=11)\n    ax.set_xlabel('Reading')\n    ax.grid(alpha=0.3)\n\naxes[0].set_ylabel('Density')\n\n# Shared legend\nhandles = [\n    plt.Line2D([0], [0], color=colors['Normal'], lw=6, alpha=0.7),\n    plt.Line2D([0], [0], color=colors['Diabetes'], lw=6, alpha=0.7)\n]\nlabels = ['Normal', 'Diabetes']\n\nfig.legend(\n    handles,\n    labels,\n    loc='upper center',\n    ncol=2,\n    fontsize=11,\n    frameon=False,\n    bbox_to_anchor=(0.5, 1.12)\n)\n\nfig.suptitle(\n    'Raw Sensor Distributions by Diagnosis',\n    fontsize=14,\n    fontweight='bold',\n    y=1.22\n)\n\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:32:31.155468Z","iopub.execute_input":"2025-12-09T18:32:31.155709Z","iopub.status.idle":"2025-12-09T18:32:32.626579Z","shell.execute_reply.started":"2025-12-09T18:32:31.155690Z","shell.execute_reply":"2025-12-09T18:32:32.625439Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Sensor-Level Signal and Overlap\n\nRaw sensor distributions reveal that individual sensors exhibit partial but overlapping separation between normal and diabetic samples. Several sensors show clear shifts in central tendency or spread, while others display substantial overlap. This pattern indicates the presence of genuine disease-related signal at the single-sensor level, but also confirms that no individual sensor is sufficient for reliable classification. The observed overlap supports realistic expectations for biological data and motivates multivariate modeling approaches that integrate across sensors rather than relying on isolated biomarkers.","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# Signal and Structure\n# ============================================================================\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Sensor Distributions (summary via effect sizes only)\neffect_sizes = {}\nfor col in sensor_cols:\n    n = df[df['Diagnosis']=='Normal'][col]\n    d = df[df['Diagnosis']=='Diabetes'][col]\n    pooled = np.sqrt((n.var() + d.var()) / 2)\n    effect_sizes[col] = (d.mean() - n.mean()) / pooled\n\npd.Series(effect_sizes).sort_values(key=abs).plot.barh(\n    ax=axes[0], color='steelblue', edgecolor='black'\n)\naxes[0].set_title('Sensor Effect Sizes (Cohen’s d)')\naxes[0].axvline(0, color='black', lw=1)\naxes[0].grid(axis='x', alpha=0.3)\n\n# PCA Projection\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\nfor label, c in zip(class_names, ['#3498db', '#e74c3c']):\n    m = df['Diagnosis'] == label\n    axes[1].scatter(X_pca[m,0], X_pca[m,1], c=c, label=label,\n                    alpha=0.6, edgecolors='white', s=50)\n\naxes[1].set_title('PCA Projection')\naxes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\naxes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\naxes[1].legend()\naxes[1].grid(alpha=0.3)\n\n# t-SNE Projection\ntsne = TSNE(n_components=2, random_state=SEED, perplexity=30)\nX_tsne = tsne.fit_transform(X_scaled)\n\nfor label, c in zip(class_names, ['#3498db', '#e74c3c']):\n    m = df['Diagnosis'] == label\n    axes[2].scatter(X_tsne[m,0], X_tsne[m,1], c=c, label=label,\n                    alpha=0.6, edgecolors='white', s=50)\n\naxes[2].set_title('t-SNE Projection')\naxes[2].legend()\naxes[2].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:32:32.627769Z","iopub.execute_input":"2025-12-09T18:32:32.628107Z","iopub.status.idle":"2025-12-09T18:32:38.169538Z","shell.execute_reply.started":"2025-12-09T18:32:32.628077Z","shell.execute_reply":"2025-12-09T18:32:38.168425Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Multivariate Structure and Separability\n\nSensor-level effect size analysis shows that discriminative power is unevenly distributed across the array. A small subset of sensors (notably TGS2611 and TGS2610) exhibits large effect sizes, while the remaining sensors contribute weaker but complementary signal. This confirms that disease information is present at the individual sensor level but concentrated rather than uniform, motivating representation learning approaches that can amplify informative sensors while attenuating noise.\n\nLower-dimensional projections clarify how these signals combine. PCA reveals partial class separation along the dominant variance directions, with the first two components accounting for the majority of total variance. Disease-related effects therefore align with global structure rather than residual noise dimensions, though substantial overlap remains, indicating incomplete linear separability in raw sensor space.\n\nNonlinear visualization with t-SNE further demonstrates coherent local structure, with samples of the same diagnosis forming compact neighborhoods despite global overlap. While t-SNE is used here only for qualitative assessment, the consistency of local clustering supports the presence of structured, multivariate disease signal. Together, these results show that breath sensor data exhibits low-dimensional, distributed, and biologically plausible structure, justifying downstream use of compact nonlinear representation learning rather than reliance on individual sensors or purely linear models.","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# Dimensionality and Interperability\n# ============================================================================\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Cumulative Variance\n\npca_full = PCA().fit(X_scaled)\ncum_var = np.cumsum(pca_full.explained_variance_ratio_)\n\naxes[0].plot(range(1,len(cum_var)+1), cum_var, 'o-', lw=2)\naxes[0].axhline(0.95, ls='--', color='gray')\naxes[0].set_title('Cumulative Variance (PCA)')\naxes[0].set_xlabel('Components')\naxes[0].set_ylabel('Variance Explained')\naxes[0].grid(alpha=0.3)\n\n# Top-2 Sensor Scatter\n\ntop2 = sorted(effect_sizes, key=lambda k: abs(effect_sizes[k]), reverse=True)[:2]\nfor label, c in zip(class_names, ['#3498db', '#e74c3c']):\n    sub = df[df['Diagnosis']==label]\n    axes[1].scatter(sub[top2[0]], sub[top2[1]],\n                    label=label, c=c, alpha=0.6, edgecolors='white')\n\naxes[1].set_title('Top 2 Sensors (Raw Space)')\naxes[1].set_xlabel(top2[0])\naxes[1].set_ylabel(top2[1])\naxes[1].legend()\naxes[1].grid(alpha=0.3)\n\n# Logistic Regression Coefficients\n\nlr = LogisticRegression(max_iter=1000)\nlr.fit(X_scaled, y_encoded)\ncoef_df = pd.Series(lr.coef_[0], index=sensor_cols).sort_values()\n\ncoef_df.plot.barh(ax=axes[2], color=['#3498db' if v<0 else '#e74c3c' for v in coef_df])\naxes[2].set_title('Logistic Regression Coefficients')\naxes[2].axvline(0, color='black')\naxes[2].grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:32:38.172232Z","iopub.execute_input":"2025-12-09T18:32:38.172585Z","iopub.status.idle":"2025-12-09T18:32:38.958204Z","shell.execute_reply.started":"2025-12-09T18:32:38.172561Z","shell.execute_reply":"2025-12-09T18:32:38.957264Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Low-Dimensional and Linear Structure\n\nCumulative variance analysis indicates strong redundancy in the breath sensor signals: the first two principal components capture the majority of total variance, with additional components contributing diminishing returns. This low intrinsic dimensionality supports the use of compact latent representations rather than high-capacity models operating directly in raw sensor space.\n\nProjection onto the two most discriminative sensors (TGS2611 and TGS2610) shows that meaningful class separation is already present in the original measurements. Although overlap remains, disease and normal samples occupy distinct regions, demonstrating that downstream model performance is grounded in genuine sensor signal rather than artifacts introduced by representation learning.\n\nA regularized logistic regression baseline identifies a similar subset of influential sensors, with coefficient magnitudes aligning closely with univariate effect sizes. This agreement across univariate statistics, raw-space projections, and linear multivariate modeling suggests that more expressive models primarily refine existing structure rather than hallucinate separability. Residual overlap in linear projections, together with known nonlinear responses of metal-oxide sensors, motivates the use of nonlinear representation learning to capture higher-order structure while preserving interpretability.","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# Interperability and Structual Change\n# ============================================================================\n\n# Top-2 Sensor Scatter\n\ntop2 = sorted(effect_sizes.items(), key=lambda x: abs(x[1]), reverse=True)[:2]\nsx, sy = top2[0][0], top2[1][0]\n\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\nfor label in class_names:\n    subset = df[df[\"Diagnosis\"] == label]\n    axes[0].scatter(subset[sx], subset[sy],\n                    label=label, alpha=0.6, color=colors[label])\n\naxes[0].set_xlabel(sx)\naxes[0].set_ylabel(sy)\naxes[0].set_title(\"Top-2 Sensor Scatter\")\naxes[0].legend()\naxes[0].grid(alpha=0.3)\n\n# Logistic Regression Coefficients\n\nlr = LogisticRegression(max_iter=1000, random_state=SEED)\nlr.fit(X_scaled, y_encoded)\ncoef_df = pd.DataFrame({\"Sensor\": sensor_cols, \"Coefficient\": lr.coef_[0]})\ncoef_df = coef_df.sort_values(\"Coefficient\")\n\naxes[1].barh(coef_df[\"Sensor\"], coef_df[\"Coefficient\"],\n             color=[\"#e74c3c\" if c > 0 else \"#3498db\" for c in coef_df[\"Coefficient\"]])\naxes[1].axvline(0, color=\"black\", linewidth=0.8)\naxes[1].set_title(\"Logistic Regression Coefficients\")\n\n# Correlation Difference Heatmap\n\ncorr_diff = df_disease.corr() - df_normal.corr()\nmask = np.triu(np.ones_like(corr_diff, dtype=bool), k=1)\n\nsns.heatmap(corr_diff, mask=mask, cmap=\"RdBu_r\", center=0,\n            annot=True, fmt=\".2f\", square=True, ax=axes[2])\naxes[2].set_title(\"Correlation Shift (Disease − Normal)\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:32:38.959195Z","iopub.execute_input":"2025-12-09T18:32:38.959490Z","iopub.status.idle":"2025-12-09T18:32:39.994176Z","shell.execute_reply.started":"2025-12-09T18:32:38.959468Z","shell.execute_reply":"2025-12-09T18:32:39.992844Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Inter-Sensor Structure and Attribution\n\nJoint analysis of raw sensor scatter, linear attribution, and correlation shifts triangulates the source of separability observed in downstream models. Projection onto TGS2611 and TGS2610, the most discriminative sensors, already reveals meaningful class structure in raw measurements. While overlap remains, samples from each class occupy distinct regions, confirming that disease signal exists prior to representation learning.\n\nLogistic regression reinforces this conclusion: sensors with large univariate effect sizes receive the strongest linear weights, indicating agreement between marginal statistics and multivariate attribution. This consistency suggests that more complex models primarily refine existing structure rather than manufacturing separability.\n\nBeyond marginal effects, correlation difference analysis shows that disease alters inter-sensor relationships. Several sensor pairs exhibit substantial shifts in correlation magnitude between normal and disease groups, indicating changes in joint response behavior rather than isolated sensor deviations. Such shifts may reflect altered metabolic coupling or sensor response regimes under disease conditions.\n\nTaken together, these findings indicate that classification performance arises from structured changes in sensor behavior, both in individual responses and their interactions, motivating latent representations that can capture relational structure rather than treating sensors as independent channels.","metadata":{}},{"cell_type":"markdown","source":"## Architecture\n\n#### Convolutional Neural Networks\n\nConvolutional neural networks assume that neighboring inputs share semantic meaning and that learned feature detectors can be applied consistently across positions (translational invariance). This inductive bias is highly effective for images, spectrograms, and other data with meaningful spatial structure.\n\nAlthough a vector of sensor values can technically be processed using one-dimensional convolutions, doing so implicitly assumes that adjacent sensors form reusable local patterns. In this dataset, sensor adjacency is arbitrary and does not correspond to physical proximity or functional similarity. Sliding convolutional kernels across unordered sensor measurements therefore does not preserve semantic meaning and provides no clear advantage over fully connected layers. As a result, CNN-based architectures would introduce inductive bias unsupported by the data.\n\nCNNs would become appropriate if sensor measurements were represented as time–frequency images, derived from dense spatial sensor arrays, or otherwise embedded in a structured spatial domain.\n\n#### Recurrent Neural Networks\n\nRecurrent neural networks, including LSTM variants, are designed to model ordered sequences in which the interpretation of a given input depends on previous elements. In the present dataset, each observation represents an independent measurement rather than a sequence, and there is no intrinsic temporal ordering among sensor values.\n\nFeeding sensor readings into an RNN would therefore impose artificial sequential structure, encouraging the model to learn dependencies driven by arbitrary feature ordering rather than meaningful dynamics. This additional complexity offers no benefit in the absence of temporal information and risks learning spurious relationships.\n\nRNN-based approaches would be well suited to extensions of this work involving continuous breath monitoring, repeated measurements over time, or modeling of sensor response dynamics within individual breath samples.\n\n#### Feedforward Neural Networks\n\nGiven these considerations, feedforward neural networks were selected as the primary modeling approach. Feedforward architectures impose minimal structural assumptions, treating inputs as unordered feature vectors and allowing interactions among sensors to be learned directly from the data. This flexibility is particularly appropriate when features lack inherent spatial or temporal structure and when feature relationships are not known a priori.\n\nThe exploratory analysis supports this selection. Logistic regression achieves strong baseline performance, indicating approximate linear separability, while correlation analysis reveals inter-sensor dependencies that more expressive models can exploit. Feedforward networks provide sufficient capacity to capture these interactions without introducing unnecessary inductive bias.\n\n#### Design Choices\n\nDataset size and structure can further guide architectural decisions. With six input features and a limited sample size (~10³ observations), deep networks are unlikely to provide additional representational benefit and may increase overfitting risk. Imagine the configuration of the form Input (6) → Hidden (32–64) → Hidden (16–32) → Output (1).  Shallow architectures with two or three hidden layers strike an appropriate balance between expressiveness and generalization.\n\nWider early layers capture feature interactions, while progressively narrower layers compress toward the decision boundary. Principal component analysis indicates that around three latent dimensions will capture the majority of variance, suggesting substantial feature redundancy. To address this we can employ Bottleneck arcitecture. Imagine the configuration Input (6) → Encoder (32 → 16) → Bottleneck (3–4) → Classifier / Decoder. The bottleneck constrains model capacity, promotes regularization, and enables interpretability by encouraging compact latent representations that may correspond to underlying physiological factors. Given the modest dataset size, multiple regularization strategies were applied, including dropout, L2 weight decay, early stopping based on validation performance, and batch normalization to stabilize training.\n\nBoth of these complementary models variants will be explored. The first, a supervised feedforward classifier optimized directly for disease discrimination.  The other was an autoencoder-based model that learns unsupervised latent structure prior to classification. This comparison assesses whether reconstruction-based representations improve robustness and generalization. These architectures aligns model assumptions with the structure of the data. Convolutional and recurrent architectures rely on spatial or sequential dependencies that are absent in unordered, static sensor measurements. Feedforward networks, by contrast, impose minimal inductive bias and are well suited for tabular data, allowing sensor interactions to be learned directly. The inclusion of a bottleneck layer further reflects the empirically observed low intrinsic dimensionality, enabling effective regularization and interpretability while avoiding unnecessary model complexity.","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# CELL 5: PYTORCH MODEL DEFINITIONS\n# ============================================================================\n\n# Autoencoder for Unsupervised Feature Learning + Reconstruction\n\nclass Autoencoder(nn.Module):\n    \"\"\"\n    Autoencoder for unsupervised feature learning and reconstruction.\n    \n    Architecture:\n        Encoder: input_dim → 32 → 16 → bottleneck_dim\n        Decoder: bottleneck_dim → 16 → 32 → input_dim\n    \n    Bottleneck provides compressed interpretable features.\n    Reconstruction loss ensures features capture essential info.\n    \"\"\"\n    \n    def __init__(self, input_dim, bottleneck_dim=8, dropout_rate=0.3):\n        super(Autoencoder, self).__init__()\n        \n        self.bottleneck_dim = bottleneck_dim\n        \n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 32),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            \n            nn.Linear(32, 16),\n            nn.BatchNorm1d(16),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            \n            nn.Linear(16, bottleneck_dim),\n            nn.BatchNorm1d(bottleneck_dim),\n            nn.ReLU()\n        )\n        \n        # Decoder\n        self.decoder = nn.Sequential(\n            nn.Linear(bottleneck_dim, 16),\n            nn.BatchNorm1d(16),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            \n            nn.Linear(16, 32),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            \n            nn.Linear(32, input_dim)\n        )\n    \n    def encode(self, x):\n        return self.encoder(x)\n    \n    def decode(self, z):\n        return self.decoder(z)\n    \n    def forward(self, x):\n        z = self.encode(x)\n        reconstructed = self.decode(z)\n        return reconstructed, z\n\n\n# Classifier with Bottleneck\n\nclass BottleneckClassifier(nn.Module):\n    \"\"\"\n    Feedforward classifier with bottleneck layer for interpretable features.\n    \n    Architecture:\n        input_dim → 32 → 16 → bottleneck_dim → 1 (sigmoid)\n    \n    The bottleneck forces information compression, making features interpretable.\n    \"\"\"\n    \n    def __init__(self, input_dim, bottleneck_dim=8, dropout_rate=0.3):\n        super(BottleneckClassifier, self).__init__()\n        \n        self.bottleneck_dim = bottleneck_dim\n        \n        # Feature extraction layers\n        self.features = nn.Sequential(\n            nn.Linear(input_dim, 32),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            \n            nn.Linear(32, 16),\n            nn.BatchNorm1d(16),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            \n            nn.Linear(16, bottleneck_dim),\n            nn.BatchNorm1d(bottleneck_dim),\n            nn.ReLU()\n        )\n        \n        # Classification head\n        self.classifier = nn.Linear(bottleneck_dim, 1)\n    \n    def get_bottleneck_features(self, x):\n        return self.features(x)\n    \n    def forward(self, x):\n        features = self.features(x)\n        logits = self.classifier(features)\n        return logits, features\n\n\n# Combined Model (Autoencoder + Classifier)\n\nclass ENoseNet(nn.Module):\n    \"\"\"\n    Combined model with autoencoder (reconstruction) and classifier.\n    \n    Enables:\n    1. Unsupervised pretraining via reconstruction\n    2. Supervised fine-tuning for classification\n    3. Both reconstruction and classification visualization\n    \"\"\"\n    \n    def __init__(self, input_dim, bottleneck_dim=8, dropout_rate=0.3):\n        super(ENoseNet, self).__init__()\n        \n        self.bottleneck_dim = bottleneck_dim\n        \n        # Shared encoder\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 32),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            \n            nn.Linear(32, 16),\n            nn.BatchNorm1d(16),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            \n            nn.Linear(16, bottleneck_dim),\n            nn.BatchNorm1d(bottleneck_dim),\n            nn.ReLU()\n        )\n        \n        # Decoder for reconstruction\n        self.decoder = nn.Sequential(\n            nn.Linear(bottleneck_dim, 16),\n            nn.BatchNorm1d(16),\n            nn.ReLU(),\n            \n            nn.Linear(16, 32),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n            \n            nn.Linear(32, input_dim)\n        )\n        \n        # Classifier head\n        self.classifier = nn.Linear(bottleneck_dim, 1)\n    \n    def encode(self, x):\n        return self.encoder(x)\n    \n    def decode(self, z):\n        return self.decoder(z)\n    \n    def forward(self, x, return_reconstruction=False):\n        z = self.encode(x)\n        logits = self.classifier(z)\n        \n        if return_reconstruction:\n            reconstruction = self.decode(z)\n            return logits, z, reconstruction\n        \n        return logits, z\n\n\n# Print architecture\nprint(\"\\nModel Architecture:\")\nprint(\"─\"*70)\nmodel_example = ENoseNet(input_dim=6, bottleneck_dim=8)\nprint(model_example)\n\n# Parameter count\ntotal_params = sum(p.numel() for p in model_example.parameters())\ntrainable_params = sum(p.numel() for p in model_example.parameters() if p.requires_grad)\nprint(f\"\\nTotal Parameters: {total_params:,}\")\nprint(f\"Trainable Parameters: {trainable_params:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:32:39.995509Z","iopub.execute_input":"2025-12-09T18:32:39.995864Z","iopub.status.idle":"2025-12-09T18:32:40.026153Z","shell.execute_reply.started":"2025-12-09T18:32:39.995827Z","shell.execute_reply":"2025-12-09T18:32:40.024860Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 6: DATA PREPARATION FOR PYTORCH\n# ============================================================================\n\nprint(\"=\"*70)\nprint(\"5. DATA PREPARATION\")\nprint(\"=\"*70)\n\n# Prepare numpy arrays\n\nX = df[sensor_cols].values.astype(np.float32)\ny = le.fit_transform(df['Diagnosis'].values).astype(np.float32)\n\n# Train/Val/Test split (60/20/20)\nX_temp, X_test, y_temp, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=SEED, stratify=y\n)\nX_train, X_val, y_train, y_val = train_test_split(\n    X_temp, y_temp, test_size=0.25, random_state=SEED, stratify=y_temp\n)\n\n# Scale\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train).astype(np.float32)\nX_val_scaled = scaler.transform(X_val).astype(np.float32)\nX_test_scaled = scaler.transform(X_test).astype(np.float32)\n\nprint(f\"\\nData Splits:\")\nprint(f\"  Training:   {len(X_train)} samples ({len(X_train)/len(X):.0%})\")\nprint(f\"  Validation: {len(X_val)} samples ({len(X_val)/len(X):.0%})\")\nprint(f\"  Test:       {len(X_test)} samples ({len(X_test)/len(X):.0%})\")\n\n# Create PyTorch datasets and dataloaders\n\n# Convert to tensors\nX_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\nX_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\ny_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\nX_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n\n# Create datasets\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\nval_dataset = TensorDataset(X_val_tensor, y_val_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n# Create dataloaders\nBATCH_SIZE = 32\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nprint(f\"\\nDataLoaders created with batch_size={BATCH_SIZE}\")\nprint(f\"  Training batches: {len(train_loader)}\")\nprint(f\"  Validation batches: {len(val_loader)}\")\nprint(f\"  Test batches: {len(test_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:32:40.027320Z","iopub.execute_input":"2025-12-09T18:32:40.027675Z","iopub.status.idle":"2025-12-09T18:32:40.060070Z","shell.execute_reply.started":"2025-12-09T18:32:40.027651Z","shell.execute_reply":"2025-12-09T18:32:40.059073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 7: TRAINING FUNCTIONS\n# ============================================================================\n\n# Training function with both reconstruction and classification losses\ndef train_epoch(model, train_loader, optimizer, device, alpha_recon=0.3):\n    \"\"\"\n    Train for one epoch with combined loss:\n    L_total = L_classification + alpha * L_reconstruction\n    \"\"\"\n    model.train()\n    total_loss = 0\n    total_cls_loss = 0\n    total_recon_loss = 0\n    correct = 0\n    total = 0\n    \n    criterion_cls = nn.BCEWithLogitsLoss()\n    criterion_recon = nn.MSELoss()\n    \n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        \n        optimizer.zero_grad()\n        \n        # Forward pass\n        logits, features, reconstruction = model(X_batch, return_reconstruction=True)\n        \n        # Losses\n        loss_cls = criterion_cls(logits, y_batch)\n        loss_recon = criterion_recon(reconstruction, X_batch)\n        loss = loss_cls + alpha_recon * loss_recon\n        \n        # Backward pass\n        loss.backward()\n        optimizer.step()\n        \n        # Statistics\n        total_loss += loss.item() * X_batch.size(0)\n        total_cls_loss += loss_cls.item() * X_batch.size(0)\n        total_recon_loss += loss_recon.item() * X_batch.size(0)\n        \n        predictions = (torch.sigmoid(logits) > 0.5).float()\n        correct += (predictions == y_batch).sum().item()\n        total += y_batch.size(0)\n    \n    return {\n        'loss': total_loss / total,\n        'cls_loss': total_cls_loss / total,\n        'recon_loss': total_recon_loss / total,\n        'accuracy': correct / total\n    }\n\n\ndef evaluate(model, data_loader, device, alpha_recon=0.3):\n    \"\"\"Evaluate model on given data loader\"\"\"\n    model.eval()\n    total_loss = 0\n    total_cls_loss = 0\n    total_recon_loss = 0\n    correct = 0\n    total = 0\n    \n    all_probs = []\n    all_labels = []\n    \n    criterion_cls = nn.BCEWithLogitsLoss()\n    criterion_recon = nn.MSELoss()\n    \n    with torch.no_grad():\n        for X_batch, y_batch in data_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            \n            logits, features, reconstruction = model(X_batch, return_reconstruction=True)\n            \n            loss_cls = criterion_cls(logits, y_batch)\n            loss_recon = criterion_recon(reconstruction, X_batch)\n            loss = loss_cls + alpha_recon * loss_recon\n            \n            total_loss += loss.item() * X_batch.size(0)\n            total_cls_loss += loss_cls.item() * X_batch.size(0)\n            total_recon_loss += loss_recon.item() * X_batch.size(0)\n            \n            probs = torch.sigmoid(logits)\n            predictions = (probs > 0.5).float()\n            correct += (predictions == y_batch).sum().item()\n            total += y_batch.size(0)\n            \n            all_probs.extend(probs.cpu().numpy().flatten())\n            all_labels.extend(y_batch.cpu().numpy().flatten())\n    \n    return {\n        'loss': total_loss / total,\n        'cls_loss': total_cls_loss / total,\n        'recon_loss': total_recon_loss / total,\n        'accuracy': correct / total,\n        'probs': np.array(all_probs),\n        'labels': np.array(all_labels)\n    }\n\n\ndef train_model(model, train_loader, val_loader, optimizer, scheduler, device,\n                n_epochs=200, patience=20, alpha_recon=0.3):\n    \"\"\"\n    Full training loop with early stopping\n    \"\"\"\n    history = {\n        'train_loss': [], 'val_loss': [],\n        'train_cls_loss': [], 'val_cls_loss': [],\n        'train_recon_loss': [], 'val_recon_loss': [],\n        'train_acc': [], 'val_acc': [],\n        'lr': []\n    }\n    \n    best_val_loss = float('inf')\n    best_model_state = None\n    patience_counter = 0\n    \n    for epoch in range(n_epochs):\n        # Train\n        train_metrics = train_epoch(model, train_loader, optimizer, device, alpha_recon)\n        \n        # Validate\n        val_metrics = evaluate(model, val_loader, device, alpha_recon)\n        \n        # Learning rate scheduling\n        if scheduler is not None:\n            scheduler.step(val_metrics['loss'])\n        \n        # Record history\n        history['train_loss'].append(train_metrics['loss'])\n        history['val_loss'].append(val_metrics['loss'])\n        history['train_cls_loss'].append(train_metrics['cls_loss'])\n        history['val_cls_loss'].append(val_metrics['cls_loss'])\n        history['train_recon_loss'].append(train_metrics['recon_loss'])\n        history['val_recon_loss'].append(val_metrics['recon_loss'])\n        history['train_acc'].append(train_metrics['accuracy'])\n        history['val_acc'].append(val_metrics['accuracy'])\n        history['lr'].append(optimizer.param_groups[0]['lr'])\n        \n        # Early stopping\n        if val_metrics['loss'] < best_val_loss:\n            best_val_loss = val_metrics['loss']\n            best_model_state = model.state_dict().copy()\n            patience_counter = 0\n        else:\n            patience_counter += 1\n        \n        # Print progress\n        if (epoch + 1) % 20 == 0 or epoch == 0:\n            print(f\"Epoch {epoch+1:3d}/{n_epochs} | \"\n                  f\"Train Loss: {train_metrics['loss']:.4f} | \"\n                  f\"Val Loss: {val_metrics['loss']:.4f} | \"\n                  f\"Train Acc: {train_metrics['accuracy']:.4f} | \"\n                  f\"Val Acc: {val_metrics['accuracy']:.4f}\")\n        \n        if patience_counter >= patience:\n            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n            break\n    \n    # Restore best model\n    if best_model_state is not None:\n        model.load_state_dict(best_model_state)\n    \n    return history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:32:40.061203Z","iopub.execute_input":"2025-12-09T18:32:40.061581Z","iopub.status.idle":"2025-12-09T18:32:40.088017Z","shell.execute_reply.started":"2025-12-09T18:32:40.061549Z","shell.execute_reply":"2025-12-09T18:32:40.086925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# VISUALIZATION: MODEL CONVERGENCE / OPTIMIZER COMPARISON (LEAN VERSION)\n# ============================================================================\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Total Loss (Classification + Reconstruction)\n\naxes[0].plot(history_sgd['train_loss'], label='SGD Train', linewidth=2, color='blue')\naxes[0].plot(history_sgd['val_loss'], label='SGD Val', linewidth=2, linestyle='--', color='blue')\naxes[0].plot(history_adam['train_loss'], label='Adam Train', linewidth=2, color='red')\naxes[0].plot(history_adam['val_loss'], label='Adam Val', linewidth=2, linestyle='--', color='red')\n\naxes[0].set_title('Total Loss (Train vs Validation)', fontweight='bold')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n\n# Training / Validation Accuracy\n\naxes[1].plot(history_sgd['train_acc'], label='SGD Train', linewidth=2, color='blue')\naxes[1].plot(history_sgd['val_acc'], label='SGD Val', linewidth=2, linestyle='--', color='blue')\naxes[1].plot(history_adam['train_acc'], label='Adam Train', linewidth=2, color='red')\naxes[1].plot(history_adam['val_acc'], label='Adam Val', linewidth=2, linestyle='--', color='red')\n\naxes[1].set_title('Training and Validation Accuracy', fontweight='bold')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Accuracy')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n\n# Test Set Performance\n\nsgd_metrics = evaluate(model_sgd, test_loader, device)\nadam_metrics = evaluate(model_adam, test_loader, device)\n\nmetrics_comparison = {\n    'SGD': [\n        sgd_metrics['accuracy'],\n        auc(*roc_curve(sgd_metrics['labels'], sgd_metrics['probs'])[:2])\n    ],\n    'Adam': [\n        adam_metrics['accuracy'],\n        auc(*roc_curve(adam_metrics['labels'], adam_metrics['probs'])[:2])\n    ]\n}\n\nx = np.arange(2)\nwidth = 0.35\n\naxes[2].bar(\n    x - width / 2, metrics_comparison['SGD'],\n    width, label='SGD', color='blue', edgecolor='black'\n)\naxes[2].bar(\n    x + width / 2, metrics_comparison['Adam'],\n    width, label='Adam', color='red', edgecolor='black'\n)\n\naxes[2].set_xticks(x)\naxes[2].set_xticklabels(['Accuracy', 'AUC'])\naxes[2].set_ylim(0.9, 1.01)\naxes[2].set_ylabel('Score')\naxes[2].set_title('Test Set Performance', fontweight='bold')\naxes[2].legend()\n\n# Value labels\nfor i, v in enumerate(metrics_comparison['SGD']):\n    axes[2].text(i - width/2, v + 0.005, f'{v:.3f}', ha='center', fontsize=10)\nfor i, v in enumerate(metrics_comparison['Adam']):\n    axes[2].text(i + width/2, v + 0.005, f'{v:.3f}', ha='center', fontsize=10)\n\nplt.tight_layout()\nplt.savefig('fig4_training_convergence.png', dpi=150, bbox_inches='tight')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:32:40.089546Z","iopub.execute_input":"2025-12-09T18:32:40.090576Z","iopub.status.idle":"2025-12-09T18:32:41.611406Z","shell.execute_reply.started":"2025-12-09T18:32:40.090532Z","shell.execute_reply":"2025-12-09T18:32:41.610119Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Both SGD with momentum and Adam converge reliably to high-performing solutions. Adam reduces loss more quickly early in training, while SGD converges more smoothly with closely aligned training and validation losses throughout. Classification loss drops rapidly, indicating that the decision boundary is learned early, consistent with the strong class separability observed during exploratory analysis. Reconstruction loss declines more gradually as the latent representation is refined, reflecting its broader objective. Identical test performance suggests that overall gains are driven more by model capacity and data separability than by optimizer choice. SGD was therefore selected due to its slightly lower validation loss and more stable late-stage convergence.","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# CELL 9: RECONSTRUCTION VISUALIZATION\n# ============================================================================\n\nmodel.eval()\n\n# Get reconstructions for test set\nwith torch.no_grad():\n    X_test_tensor_device = X_test_tensor.to(device)\n    _, features_test, reconstructions = model(\n        X_test_tensor_device, return_reconstruction=True\n    )\n\n    reconstructions = reconstructions.cpu().numpy()\n    features_test = features_test.cpu().numpy()\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Reconstruction error by class\n\nrecon_errors = np.mean((X_test_scaled - reconstructions) ** 2, axis=1)\nrecon_errors_normal = recon_errors[y_test == 0]\nrecon_errors_disease = recon_errors[y_test == 1]\n\naxes[0].hist(\n    recon_errors_normal, bins=30, alpha=0.6,\n    label=class_names[0], color='green',\n    edgecolor='black', density=True\n)\naxes[0].hist(\n    recon_errors_disease, bins=30, alpha=0.6,\n    label=class_names[1], color='red',\n    edgecolor='black', density=True\n)\n\naxes[0].set_title('Reconstruction Error by Class', fontweight='bold')\naxes[0].set_xlabel('Mean Squared Error')\naxes[0].set_ylabel('Density')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n\n# Original vs reconstructed (single sensor)\n\nsensor_idx = 0  # first sensor\naxes[1].scatter(\n    X_test_scaled[:, sensor_idx],\n    reconstructions[:, sensor_idx],\n    c=y_test, cmap='RdYlGn',\n    alpha=0.6, edgecolors='black', linewidths=0.5\n)\n\naxes[1].plot([-3, 3], [-3, 3], 'k--', linewidth=2, label='Perfect reconstruction')\n\naxes[1].set_title(\n    f'{sensor_cols[sensor_idx]}: Original vs Reconstructed',\n    fontweight='bold'\n)\naxes[1].set_xlabel(f'Original {sensor_cols[sensor_idx]}')\naxes[1].set_ylabel(f'Reconstructed {sensor_cols[sensor_idx]}')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n\n# Reconstruction R² by sensor\n\nr2_scores = []\nfor i in range(len(sensor_cols)):\n    ss_res = np.sum((X_test_scaled[:, i] - reconstructions[:, i]) ** 2)\n    ss_tot = np.sum((X_test_scaled[:, i] - X_test_scaled[:, i].mean()) ** 2)\n    r2 = 1 - ss_res / ss_tot\n    r2_scores.append(r2)\n\naxes[2].bar(\n    sensor_cols, r2_scores,\n    color='forestgreen', edgecolor='black'\n)\naxes[2].axhline(\n    y=0.9, color='red', linestyle='--',\n    alpha=0.7, label='0.9 threshold'\n)\n\naxes[2].set_title('Reconstruction R² by Sensor', fontweight='bold')\naxes[2].set_ylabel('R² Score')\naxes[2].tick_params(axis='x', rotation=45)\naxes[2].legend()\naxes[2].grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.savefig('fig5_reconstruction.png', dpi=150, bbox_inches='tight')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:32:41.614814Z","iopub.execute_input":"2025-12-09T18:32:41.615087Z","iopub.status.idle":"2025-12-09T18:32:43.148755Z","shell.execute_reply.started":"2025-12-09T18:32:41.615069Z","shell.execute_reply":"2025-12-09T18:32:43.147988Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Reconstruction analysis indicates that the autoencoder’s bottleneck preserves the majority of input signal without introducing class-dependent distortion. Reconstruction error distributions for diabetic and normal samples overlap substantially, with similar mean reconstruction error, confirming that neither class is preferentially represented during encoding. At the sensor level, reconstruction quality remains consistently high, with per-sensor explained variance exceeding ~90%, supporting earlier PCA findings of low intrinsic dimensionality. These results demonstrate that compressing six sensor measurements into a low-dimensional latent space retains meaningful structure rather than collapsing inputs toward class averages. While balanced reconstruction alone does not induce class separation, it confirms that downstream classification performance arises from the geometry of the learned feature space rather than from differential reconstruction artifacts, supporting the use of the bottleneck representation as a stable and unbiased basis for disease classification.","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# CELL 10: VISUALIZATION - ROC CURVE & CONFUSION MATRIX\n# ============================================================================\n\n# Get final predictions\ntest_metrics = evaluate(model, test_loader, device)\ny_pred_prob = test_metrics['probs']\ny_pred = (y_pred_prob > 0.5).astype(int)\ny_true = test_metrics['labels'].astype(int)\n\n# Create layout\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# ROC Curve\n\nfpr, tpr, thresholds = roc_curve(y_true, y_pred_prob)\nroc_auc = auc(fpr, tpr)\n\naxes[0].plot(fpr, tpr, lw=3, color='darkorange',\n             label=f'ROC Curve (AUC = {roc_auc:.3f})')\naxes[0].fill_between(fpr, tpr, alpha=0.3, color='orange')\naxes[0].plot([0, 1], [0, 1], '--', color='navy', lw=2,\n             label='Random (AUC = 0.500)')\n\n# Optimal threshold marker\noptimal_idx = np.argmax(tpr - fpr)\naxes[0].scatter(\n    fpr[optimal_idx], tpr[optimal_idx],\n    color='red', s=80, zorder=5,\n    label=f'Optimal threshold = {thresholds[optimal_idx]:.2f}'\n)\n\naxes[0].set_xlim([0, 1])\naxes[0].set_ylim([0, 1.05])\naxes[0].set_xlabel('False Positive Rate', fontsize=12)\naxes[0].set_ylabel('True Positive Rate', fontsize=12)\naxes[0].set_title('ROC Curve', fontsize=14, fontweight='bold')\naxes[0].legend(loc='lower right', fontsize=10)\naxes[0].grid(True, alpha=0.3)\n\n# Confusion Matrix\n\ncm = confusion_matrix(y_true, y_pred)\n\nsns.heatmap(\n    cm,\n    annot=True,\n    fmt='d',\n    cmap='Blues',\n    ax=axes[1],\n    xticklabels=class_names,\n    yticklabels=class_names,\n    annot_kws={'size': 14},\n    cbar=False\n)\n\naxes[1].set_xlabel('Predicted Label', fontsize=12)\naxes[1].set_ylabel('True Label', fontsize=12)\naxes[1].set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.savefig('fig6_roc_confusion.png', dpi=150, bbox_inches='tight')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:32:43.150034Z","iopub.execute_input":"2025-12-09T18:32:43.150440Z","iopub.status.idle":"2025-12-09T18:32:44.009690Z","shell.execute_reply.started":"2025-12-09T18:32:43.150409Z","shell.execute_reply":"2025-12-09T18:32:44.008493Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Classification results show complete separation between diabetic and normal samples on the held-out test set, achieving perfect accuracy, precision, recall, F1 score, and ROC–AUC (all 1.0). The ROC curve rises vertically to perfect sensitivity at zero false positive rate, indicating deterministic separability rather than performance driven by threshold tuning. This outcome is consistent with earlier latent space and reconstruction analyses that revealed minimal class overlap. Nevertheless, perfect performance on biomedical data warrants cautious interpretation, as it may reflect limited sample diversity or test set size rather than true population-level generalizability. External validation on independent cohorts, potentially collected using different hardware, measurement protocols, or patient demographics, would be required to assess robustness under distributional shift and establish clinical applicability.","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# CELL 11: FEATURE INTERPRETATION (REFINED)\n# ============================================================================\n\n# Extract features for all data (inference only)\n\nmodel.eval()\nX_all_scaled = scaler.transform(X).astype(np.float32)\nX_all_tensor = torch.tensor(X_all_scaled, dtype=torch.float32).to(device)\n\nwith torch.no_grad():\n    _, features_all, _ = model(X_all_tensor, return_reconstruction=True)\n    features_all = features_all.cpu().numpy()\n\n# Visulizing Raw vs Learned Feature Space (PCA)\n\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\n# Raw sensor space\npca_raw = PCA(n_components=2, svd_solver='full', random_state=42)\nX_raw_pca = pca_raw.fit_transform(X_all_scaled)\n\naxes[0].scatter(X_raw_pca[y_encoded == 0, 0], X_raw_pca[y_encoded == 0, 1],\n                c='green', label=class_names[0], alpha=0.6, edgecolors='black', s=50)\naxes[0].scatter(X_raw_pca[y_encoded == 1, 0], X_raw_pca[y_encoded == 1, 1],\n                c='red', label=class_names[1], alpha=0.6, edgecolors='black', s=50)\naxes[0].set_xlabel(f'PC1 ({pca_raw.explained_variance_ratio_[0]:.1%})')\naxes[0].set_ylabel(f'PC2 ({pca_raw.explained_variance_ratio_[1]:.1%})')\naxes[0].set_title('Raw Sensor Space (PCA)', fontsize=14, fontweight='bold')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Learned bottleneck feature space\npca_learned = PCA(n_components=2, svd_solver='full', random_state=42)\nX_learned_pca = pca_learned.fit_transform(features_all)\n\naxes[1].scatter(X_learned_pca[y_encoded == 0, 0], X_learned_pca[y_encoded == 0, 1],\n                c='green', label=class_names[0], alpha=0.6, edgecolors='black', s=50)\naxes[1].scatter(X_learned_pca[y_encoded == 1, 0], X_learned_pca[y_encoded == 1, 1],\n                c='red', label=class_names[1], alpha=0.6, edgecolors='black', s=50)\naxes[1].set_xlabel(f'Learned PC1 ({pca_learned.explained_variance_ratio_[0]:.1%})')\naxes[1].set_ylabel(f'Learned PC2 ({pca_learned.explained_variance_ratio_[1]:.1%})')\naxes[1].set_title('Learned Feature Space (PCA)', fontsize=14, fontweight='bold')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n# Feature activations by class\nfeature_names = [f'F{i+1}' for i in range(bottleneck_dim)]\nfeature_means = pd.DataFrame({\n    'Feature': feature_names,\n    class_names[0]: features_all[y_encoded == 0].mean(axis=0),\n    class_names[1]: features_all[y_encoded == 1].mean(axis=0)\n})\n\nx = np.arange(bottleneck_dim)\nwidth = 0.35\naxes[2].bar(x - width/2, feature_means[class_names[0]], width,\n            label=class_names[0], color='green', edgecolor='black')\naxes[2].bar(x + width/2, feature_means[class_names[1]], width,\n            label=class_names[1], color='red', edgecolor='black')\naxes[2].set_xlabel('Bottleneck Feature')\naxes[2].set_ylabel('Mean Activation')\naxes[2].set_title('Feature Activations by Class', fontsize=14, fontweight='bold')\naxes[2].set_xticks(x)\naxes[2].set_xticklabels(feature_names)\naxes[2].legend()\naxes[2].grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.savefig('fig7_feature_separation.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# Quantify class separation improvement (silhouette score)\n\nsil_raw = silhouette_score(X_all_scaled, y_encoded)\nsil_learned = silhouette_score(features_all, y_encoded)\n\nprint(\"\\nClass Separation Comparison:\")\nprint(f\"  Raw sensor space silhouette:     {sil_raw:.3f}\")\nprint(f\"  Learned feature space silhouette:{sil_learned:.3f}\")\nprint(f\"  Absolute improvement:            {sil_learned - sil_raw:.3f}\")\n\n# Input-to-feature correlation analysis\n\ninput_feature_corr = np.zeros((len(sensor_cols), bottleneck_dim))\nfor i in range(len(sensor_cols)):\n    for j in range(bottleneck_dim):\n        input_feature_corr[i, j] = np.corrcoef(X_all_scaled[:, i], features_all[:, j])[0, 1]\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nsns.heatmap(input_feature_corr, annot=True, fmt='.2f',\n            cmap='RdBu_r', center=0, vmin=-1, vmax=1,\n            xticklabels=feature_names, yticklabels=sensor_cols, ax=axes[0])\naxes[0].set_title('Input–Feature Correlation', fontsize=14, fontweight='bold')\naxes[0].set_xlabel('Bottleneck Feature')\naxes[0].set_ylabel('Input Sensor')\n\n# Gradient-based sensor importance (class-conditional)\n\nX_tensor_grad = torch.tensor(X_all_scaled, dtype=torch.float32, requires_grad=True).to(device)\nlogits, _ = model(X_tensor_grad)\n\n# Focus gradients on disease class\nloss = logits[y_encoded == 1].sum()\nloss.backward()\n\ngradient_importance = X_tensor_grad.grad.abs().mean(dim=0).cpu().numpy()\ngradient_importance /= gradient_importance.max()\n\naxes[1].barh(sensor_cols, gradient_importance,\n             color='steelblue', edgecolor='black')\naxes[1].set_xlabel('Relative Importance (Gradient)')\naxes[1].set_title('Sensor Importance (Gradient-based)', fontsize=14, fontweight='bold')\naxes[1].grid(True, alpha=0.3, axis='x')\n\nplt.tight_layout()\nplt.savefig('fig8_feature_interpretation.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# Feature naming (interpretable summary)\n\nbiomarker_map = {\n    'TGS2610': ('Acetone, Butane', 'Diabetes, Ketoacidosis'),\n    'TGS2611': ('Methane, Acetone', 'Diabetes, GI disorders'),\n    'TGS2600': ('Hydrogen, Ethanol', 'General metabolic'),\n    'TGS2602': ('Ammonia, H₂S, VOCs', 'Kidney and liver disease'),\n    'TGS826':  ('Ammonia, Amines', 'Kidney disease, H. pylori'),\n    'TGS2620': ('Ethanol, Solvents', 'Liver disease, Diabetes'),\n}\n\nfor j in range(bottleneck_dim):\n    corrs = input_feature_corr[:, j]\n    top_idx = np.argsort(np.abs(corrs))[::-1][:2]\n    top_sensors = [sensor_cols[i] for i in top_idx]\n    top_corrs = [corrs[i] for i in top_idx]\n\n    class_diff = features_all[y_encoded == 1, j].mean() - features_all[y_encoded == 0, j].mean()\n    direction = \"↑ Disease\" if class_diff > 0.1 else \"↓ Disease\" if class_diff < -0.1 else \"≈ Similar\"\n\n    compounds = biomarker_map.get(top_sensors[0], ('Unknown', 'Unknown'))[0]\n\n    print(f\"\\n  Feature F{j+1}:\")\n    print(f\"    Dominant sensors: {top_sensors[0]} (r={top_corrs[0]:.2f}), \"\n          f\"{top_sensors[1]} (r={top_corrs[1]:.2f})\")\n    print(f\"    Putative compounds (sensor sensitivity): {compounds}\")\n    print(f\"    Class pattern: {direction} (Δ={class_diff:.2f})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:32:44.010812Z","iopub.execute_input":"2025-12-09T18:32:44.011084Z","iopub.status.idle":"2025-12-09T18:32:46.828557Z","shell.execute_reply.started":"2025-12-09T18:32:44.011065Z","shell.execute_reply":"2025-12-09T18:32:46.827688Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To evaluate whether the learned bottleneck representation captures meaningful and interpretable structure, we examined class separability, sensor associations, and feature activation patterns within the latent space. Dimensionality reduction of the bottleneck features reveals substantially improved class separation compared to the raw sensor space, consistent with the observed increase in silhouette score. This indicates that the encoder organizes samples into a geometry that more explicitly reflects disease status, rather than merely compressing input variance.\n\nAnalysis of bottleneck feature activations shows that only a subset of latent dimensions exhibits strong class-dependent behavior, while others remain relatively balanced across classes. This sparsity suggests that disease-relevant information is concentrated in specific axes of the latent space rather than diffusely encoded. Correlation analysis between bottleneck features and input sensors further indicates that latent dimensions reflect structured combinations of sensor responses, rather than dominance by individual sensors alone.\n\nTo support interpretability, gradient-based sensitivity analysis was used to identify input sensors most influential for disease prediction. These importance patterns are consistent with both linear baseline coefficients and latent-space structure, reinforcing the notion that the model’s decisions rely on physiologically meaningful signal variations captured by the sensor array.\n\nFinally, we present putative interpretations of bottleneck features based on their strongest statistical associations with individual sensors and known sensor sensitivity profiles. These interpretations are hypothesis-generating and do not constitute direct identification of specific breath biomarkers. Rather, they provide an interpretable bridge between learned representations and known chemical sensitivities of the sensor suite, offering insight into how latent dimensions may correspond to dominant response patterns. The balanced reconstruction quality across classes and consistent feature geometry support the use of the bottleneck representation as a stable, unbiased feature space for downstream classification.","metadata":{}},{"cell_type":"code","source":"kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n\naccs, aucs = [], []\n\nfor train_idx, val_idx in kfold.split(X, y_encoded):\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X[train_idx]).astype(np.float32)\n    X_val   = scaler.transform(X[val_idx]).astype(np.float32)\n\n    y_train = y_encoded[train_idx].astype(np.float32)\n    y_val   = y_encoded[val_idx].astype(np.float32)\n\n    train_loader = DataLoader(\n        TensorDataset(torch.tensor(X_train), torch.tensor(y_train).unsqueeze(1)),\n        batch_size=32, shuffle=True\n    )\n    val_loader = DataLoader(\n        TensorDataset(torch.tensor(X_val), torch.tensor(y_val).unsqueeze(1)),\n        batch_size=32, shuffle=False\n    )\n\n    model = ENoseNet(input_dim=len(sensor_cols), bottleneck_dim=8).to(device)\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.01)\n\n    train_model(model, train_loader, val_loader, optimizer, None, device,\n                n_epochs=100, patience=15, alpha_recon=0.3)\n\n    metrics = evaluate(model, val_loader, device)\n    auc_val = auc(*roc_curve(metrics['labels'], metrics['probs'])[:2])\n\n    accs.append(metrics['accuracy'])\n    aucs.append(auc_val)\n\nprint(\"\\n\" + \"─\"*50)\nprint(\"Cross-Validation Summary (5-fold)\")\nprint(\"─\"*50)\nprint(f\"Accuracy: {np.mean(accs):.3f} ± {np.std(accs):.3f}\")\nprint(f\"AUC:      {np.mean(aucs):.3f} ± {np.std(aucs):.3f}\")\nprint(\"─\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:32:46.829585Z","iopub.execute_input":"2025-12-09T18:32:46.829909Z","iopub.status.idle":"2025-12-09T18:33:11.545016Z","shell.execute_reply.started":"2025-12-09T18:32:46.829880Z","shell.execute_reply":"2025-12-09T18:33:11.543540Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Stratified five-fold cross-validation was conducted to assess the stability of the full modeling pipeline with respect to data partitioning. Preprocessing and model training were performed independently within each fold to prevent information leakage. Performance remained consistent across all folds, indicating that model behavior is not driven by a favorable train–test split but reflects structure present throughout the dataset. While cross-validation does not substitute for external validation on independent cohorts, these results provide evidence of internal stability and robustness of the learned decision boundary.","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# FINAL SUMMARY\n# ============================================================================\n\nfinal_metrics = evaluate(model, test_loader, device)\nfinal_auc = auc(*roc_curve(final_metrics['labels'], final_metrics['probs'])[:2])\n\nprint(f\"\"\"\nDataset\n  • Samples:        {len(df)}\n  • Sensors:        {len(sensor_cols)} ({\", \".join(sensor_cols)})\n  • Data Source:    {\"Synthetic\" if USING_SYNTHETIC_DATA else \"Clinical\"}\n\nModel\n  • Architecture:  Feedforward NN + Autoencoder\n  • Structure:     6 → 32 → 16 → 8 → 1 (+ decoder)\n  • Regularization Dropout(0.3), L2(0.01), BatchNorm\n\nTraining\n  • Optimizer:     {chosen_optimizer}\n  • Scheduler:     ReduceLROnPlateau\n  • Cross-Validation: 5-fold stratified (leakage-free)\n\nPerformance\n  • Test Accuracy: {final_metrics[\"accuracy\"]:.4f}\n  • Test AUC:      {final_auc:.4f}\n  • CV Accuracy:   {cv_df[\"accuracy\"].mean():.4f} ± {cv_df[\"accuracy\"].std():.4f}\n  • CV AUC:        {cv_df[\"auc\"].mean():.4f} ± {cv_df[\"auc\"].std():.4f}\n\nRepresentation Quality\n  • Mean Recon MSE: {np.mean(recon_errors):.4f}\n  • Mean Recon R²:  {np.mean(r2_scores):.4f}\n  • Silhouette:     {sil_raw:.3f} → {sil_learned:.3f}\n\"\"\")\n\ntorch.save({\n    \"model_state_dict\": model.state_dict(),\n    \"scaler_mean\": scaler.mean_,\n    \"scaler_std\": scaler.scale_,\n    \"sensor_cols\": sensor_cols,\n    \"class_names\": list(class_names),\n    \"bottleneck_dim\": bottleneck_dim\n}, \"enose_model.pth\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T18:33:11.546214Z","iopub.execute_input":"2025-12-09T18:33:11.546938Z","iopub.status.idle":"2025-12-09T18:33:11.577162Z","shell.execute_reply.started":"2025-12-09T18:33:11.546901Z","shell.execute_reply":"2025-12-09T18:33:11.575933Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## What This Project Demonstrates\n\nThis work demonstrates that multivariate breath sensor data contain meaningful internal structure that can be uncovered through careful exploratory analysis and effectively exploited through representation learning. Dimensionality reduction reveals substantial redundancy in sensor responses, motivating compact latent representations that capture relevant variation. Within this setting, feedforward neural networks with an autoencoder bottleneck are particularly well suited to the static, low-dimensional, tabular nature of the data, as they learn inter-sensor relationships directly without imposing inappropriate spatial or temporal inductive biases.\n\nThe learned latent representations improve class separability relative to the raw sensor space while preserving input structure, as evidenced by increased silhouette scores and high reconstruction fidelity. Model performance remains stable across optimization strategies and stratified cross-validation folds, indicating robustness to training conditions and data partitioning. Interpretability analyses, including reconstruction diagnostics, feature correlations, and gradient-based importance measures, provide insight into how sensor patterns contribute to classification without overclaiming biological specificity. Collectively, these results establish a reproducible and interpretable modeling framework for scent-based health screening under realistic data constraints.","metadata":{}},{"cell_type":"markdown","source":"## Limitations and Next Steps\n\nSeveral limitations warrant consideration. The analysis is based on a single dataset with binary disease labels and a modest sample size, which constrains conclusions regarding generalizability across populations, disease subtypes, and acquisition settings. Although classification performance is consistently near-perfect across stratified cross-validation folds, such results may reflect limited heterogeneity within the study population rather than robust discriminability under real-world conditions. In addition, potentially important confounding factors, including recent diet, smoking status, medications, and comorbidities, were not controlled and may influence breath volatile organic compound (VOC) profiles. Temporal dynamics of sensor responses are not modeled, and sensor drift, an important challenge for electronic-nose deployments, is acknowledged but not explicitly addressed. While reconstruction quality and feature analyses suggest that the learned representations capture physiologically relevant structure, the resulting latent features should be regarded as hypothesis-generating rather than definitive indicators of specific biological mechanisms or biomarkers.\n\nFuture work should prioritize external validation using independent cohorts that better reflect population diversity in demographics, disease severity, and clinical context. Integrating electronic-nose sensor arrays with heterogeneous datasets spanning complementary sensing modalities, such as higher-resolution breathomics platforms (e.g., GC–MS–derived VOC profiles) and clinical or wearable data, would enable evaluation of robustness across data sources and acquisition protocols. Beyond classification performance, developing mechanistic interpretability that links learned latent features to specific VOCs or metabolic pathways would strengthen clinical relevance. Incorporating temporal sensor dynamics, extending the framework to multi-class disease phenotyping, and applying calibration or transfer methods to mitigate sensor drift represent additional steps toward practical, deployable healthcare applications.","metadata":{}}]}